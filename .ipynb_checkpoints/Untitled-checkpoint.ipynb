{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4667e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import copy\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import letterbox, img_formats, vid_formats, LoadImages, LoadStreams\n",
    "from utils.general import check_img_size, non_max_suppression_face, apply_classifier, scale_coords, xyxy2xywh, \\\n",
    "    strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "from face_embedding.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9d34e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--weights', nargs='+', type=str, default='weights/yolov5n-face.pt', help='model.pt path(s)')\n",
    "parser.add_argument('--folderdataset_dir', default='data_recognition/raw', type=str,\n",
    "                    help='Path to Face Image Folder Dataset')\n",
    "parser.add_argument('--save_path', default='data_recognition/preprossced', type=str,\n",
    "                    help='Path for saving folder')\n",
    "parser.add_argument('--cpu-mode', action='store_true', help='Use the CPU.')\n",
    "\n",
    "args=parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34dde8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(weights, device):\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    return model\n",
    "\n",
    "\n",
    "def scale_coords_landmarks(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2, 4, 6, 8]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3, 5, 7, 9]] -= pad[1]  # y padding\n",
    "    coords[:, :10] /= gain\n",
    "    #clip_coords(coords, img0_shape)\n",
    "    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n",
    "    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n",
    "    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n",
    "    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n",
    "    coords[:, 4].clamp_(0, img0_shape[1])  # x3\n",
    "    coords[:, 5].clamp_(0, img0_shape[0])  # y3\n",
    "    coords[:, 6].clamp_(0, img0_shape[1])  # x4\n",
    "    coords[:, 7].clamp_(0, img0_shape[0])  # y4\n",
    "    coords[:, 8].clamp_(0, img0_shape[1])  # x5\n",
    "    coords[:, 9].clamp_(0, img0_shape[0])  # y5\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44691929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "device=torch.device(\"cuda\" if (not args.cpu_mode) &(torch.cuda.is_available()) else \"cpu\") \n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((640, 640)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "data_inf = FaceImageFolderDataset(root= args.folderdataset_dir, transform = transform)\n",
    "\n",
    "model = load_model(args.weights, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4dea3ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  ...\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]]\n",
      "\n",
      " [[ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  ...\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]]\n",
      "\n",
      " [[ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  [ 77 108 141]\n",
      "  ...\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]\n",
      "  [ 44  81  89]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  ...\n",
      "  [ 34  54  72]\n",
      "  [ 34  54  72]\n",
      "  [ 36  53  72]]\n",
      "\n",
      " [[ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  ...\n",
      "  [ 34  54  72]\n",
      "  [ 34  54  72]\n",
      "  [ 36  53  72]]\n",
      "\n",
      " [[ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  [ 46  64  87]\n",
      "  ...\n",
      "  [ 36  53  72]\n",
      "  [ 36  53  72]\n",
      "  [ 36  53  72]]]\n"
     ]
    }
   ],
   "source": [
    "conf_thres = 0.6\n",
    "iou_thres = 0.5\n",
    "for idx, (img, _) in enumerate(data_inf):\n",
    "    img=img.to(device)\n",
    "    pred = model(img.unsqueeze(0))[0]\n",
    "    pred = non_max_suppression_face(pred, conf_thres, iou_thres)[0]\n",
    "    if len(pred) !=1:\n",
    "        logging.warning(f\"Image{data_inf.img_paths[idx]} error (no face or more than 1 face) !\")\n",
    "        continue\n",
    "    #rescale bb to orginial size\n",
    "    im0= cv2.imread(data_inf.img_paths[idx])\n",
    "    print(im0)\n",
    "    pred[:, :4] = scale_coords(img.shape[1:], pred[:, :4], im0.shape).round()\n",
    "    xyxy = pred[0, :4].view(-1).tolist()\n",
    "    x1,y1,x2,y2 = xyxy\n",
    "\n",
    "    cropped_face= im0[int(y1):int(y2),int(x1):int(x2)]\n",
    "    save_path=os.path.join(args.save_path, data_inf.img_ids[idx])\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(save_path, data_inf.img_paths[idx].split('\\\\')[-1]), cropped_face)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c912b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python align_data.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fd00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
