{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013edd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import dataset\n",
    "import argparse\n",
    "from model.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017fc513",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hehe\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mFaceImageFolderDataset(root\u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mfolderdataset_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "hehe= dataset.FaceImageFolderDataset(root= args.folderdataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "la=10\n",
    "ua=110\n",
    "lm=0.45\n",
    "um=0.8\n",
    "lg=35\n",
    "\n",
    "# settings\n",
    "MODEL_ARC='alexnet'\n",
    "OUTPUT='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -u trainer.py --backbone alexnet --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold test --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44cd1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> parse the args ...\u001b[0m\n",
      "{'arc_scale': 64,\n",
      " 'backbone': 'alexnet',\n",
      " 'batch_size': 1,\n",
      " 'embedding_size': 512,\n",
      " 'epochs': 25,\n",
      " 'folderdataset_dir': 'datasets/raw',\n",
      " 'l_a': 10,\n",
      " 'l_margin': 0.45,\n",
      " 'lambda_g': 20,\n",
      " 'last_fc_size': 2,\n",
      " 'lr': 0.1,\n",
      " 'lr_drop_epoch': [30, 60, 90],\n",
      " 'lr_drop_ratio': 0.1,\n",
      " 'momentum': 0.9,\n",
      " 'pretrained_backbone': 'True',\n",
      " 'print_freq': 2,\n",
      " 'pth_save_epoch': 1,\n",
      " 'pth_save_fold': 'test',\n",
      " 'start_epoch': 0,\n",
      " 'train_list': '',\n",
      " 'u_a': 110,\n",
      " 'u_margin': 0.8,\n",
      " 'vis_mag': 1,\n",
      " 'weight_decay': 0.0001,\n",
      " 'workers': 1}\n",
      "\u001b[31mmin lambda g is 22.586666666666673, currrent lambda is 20\u001b[0m\n",
      "\u001b[32m=> torch version : 1.12.1\u001b[0m\n",
      "\u001b[32m=> ngpus : 1\u001b[0m\n",
      "\u001b[32m=> modeling the network ...\u001b[0m\n",
      "\u001b[32m=> building the oprimizer ...\u001b[0m\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\u001b[32m=> building the dataloader ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 265, in <module>\n",
      "    main(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 110, in main\n",
      "    main_worker(ngpus_per_node, args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 134, in main_worker\n",
      "    train_loader = dataset.train_loader(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\datasets\\dataset.py\", line 119, in train_loader\n",
      "    print(hehe)\n",
      "NameError: name 'hehe' is not defined\n"
     ]
    }
   ],
   "source": [
    "python -u trainer.py --backbone {MODEL_ARC} --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold {OUTPUT} --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1267d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone', default='iresnet18', type=str,\n",
    "                    help='backbone architechture')\n",
    "parser.add_argument('--pretrained_backbone', default=False, type=str,\n",
    "                    help='Use pretrain backbone')\n",
    "parser.add_argument('--resume', default=None, type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--folderdataset_dir', default='datasets/raw', type=str,\n",
    "                    help='Path to Face Image Folder Dataset')\n",
    "parser.add_argument('--feat_list', default='datasets/face_embdding.txt', type=str,\n",
    "                    help='Path for saveing features file')\n",
    "parser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=512, type=int, metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                    'batch size of all GPUs on the current node when '\n",
    "                    'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--embedding-size', default=512, type=int,\n",
    "                    help='The embedding feature size')\n",
    "parser.add_argument('--cpu-mode', action='store_true', help='Use the CPU.')\n",
    "\n",
    "args=parser.parse_args(''.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501a119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= build_backbone(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1903c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> starting face embdding...\u001b[0m\n",
      "=> embdding feature will be saved into datasets/face_embdding.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if not args.cpu_mode:\n",
    "    model = model.cuda()\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((112, 112)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "\n",
    "data_inf = dataset.FaceImageFolderDataset(root= args.folderdataset_dir, transform = transform)\n",
    "dataloader_inf=DataLoader(data_inf, \n",
    "                          batch_size= args.batch_size, \n",
    "                          num_workers= args.workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False,\n",
    ")\n",
    "cprint('=> starting face embdding...', 'green')\n",
    "cprint('=> embdding feature will be saved into {}'.format(args.feat_list))\n",
    "model.eval()\n",
    "\n",
    "file= open(args.feat_list, 'w')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(dataloader_inf):\n",
    "        imgs=imgs.to(device)\n",
    "        embedding_feats = model(imgs)\n",
    "        embedding_feats = embedding_feats.data.cpu().numpy()\n",
    "        \n",
    "        for feat, label in zip(embedding_feats, labels):\n",
    "            file.write('{} '.format(label))\n",
    "            for r in feat:\n",
    "                file.write('{} '.format(r))\n",
    "            file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbce62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e0701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
