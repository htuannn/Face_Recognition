{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013edd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import dataset\n",
    "import argparse\n",
    "from model.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017fc513",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hehe\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mFaceImageFolderDataset(root\u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mfolderdataset_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "hehe= dataset.FaceImageFolderDataset(root= args.folderdataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "la=10\n",
    "ua=110\n",
    "lm=0.45\n",
    "um=0.8\n",
    "lg=35\n",
    "\n",
    "# settings\n",
    "MODEL_ARC='alexnet'\n",
    "OUTPUT='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -u trainer.py --backbone alexnet --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold test --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44cd1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> parse the args ...\u001b[0m\n",
      "{'arc_scale': 64,\n",
      " 'backbone': 'alexnet',\n",
      " 'batch_size': 1,\n",
      " 'embedding_size': 512,\n",
      " 'epochs': 25,\n",
      " 'folderdataset_dir': 'datasets/raw',\n",
      " 'l_a': 10,\n",
      " 'l_margin': 0.45,\n",
      " 'lambda_g': 20,\n",
      " 'last_fc_size': 2,\n",
      " 'lr': 0.1,\n",
      " 'lr_drop_epoch': [30, 60, 90],\n",
      " 'lr_drop_ratio': 0.1,\n",
      " 'momentum': 0.9,\n",
      " 'pretrained_backbone': 'True',\n",
      " 'print_freq': 2,\n",
      " 'pth_save_epoch': 1,\n",
      " 'pth_save_fold': 'test',\n",
      " 'start_epoch': 0,\n",
      " 'train_list': '',\n",
      " 'u_a': 110,\n",
      " 'u_margin': 0.8,\n",
      " 'vis_mag': 1,\n",
      " 'weight_decay': 0.0001,\n",
      " 'workers': 1}\n",
      "\u001b[31mmin lambda g is 22.586666666666673, currrent lambda is 20\u001b[0m\n",
      "\u001b[32m=> torch version : 1.12.1\u001b[0m\n",
      "\u001b[32m=> ngpus : 1\u001b[0m\n",
      "\u001b[32m=> modeling the network ...\u001b[0m\n",
      "\u001b[32m=> building the oprimizer ...\u001b[0m\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\u001b[32m=> building the dataloader ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 265, in <module>\n",
      "    main(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 110, in main\n",
      "    main_worker(ngpus_per_node, args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 134, in main_worker\n",
      "    train_loader = dataset.train_loader(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\datasets\\dataset.py\", line 119, in train_loader\n",
      "    print(hehe)\n",
      "NameError: name 'hehe' is not defined\n"
     ]
    }
   ],
   "source": [
    "python -u trainer.py --backbone {MODEL_ARC} --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold {OUTPUT} --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1267d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone', default='iresnet18', type=str,\n",
    "                    help='backbone architechture')\n",
    "parser.add_argument('--pretrained_backbone', default=False, type=str,\n",
    "                    help='Use pretrain backbone')\n",
    "parser.add_argument('--resume', default=None, type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--folderdataset_dir', default='datasets/raw', type=str,\n",
    "                    help='Path to Face Image Folder Dataset')\n",
    "parser.add_argument('--feat_list', default='datasets/face_embdding.txt', type=str,\n",
    "                    help='Path for saveing features file')\n",
    "parser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=512, type=int, metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                    'batch size of all GPUs on the current node when '\n",
    "                    'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--embedding-size', default=512, type=int,\n",
    "                    help='The embedding feature size')\n",
    "parser.add_argument('--cpu-mode', action='store_true', help='Use the CPU.')\n",
    "\n",
    "args=parser.parse_args(''.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501a119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= build_backbone(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1903c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> starting face embdding...\u001b[0m\n",
      "=> embdding feature will be saved into datasets/face_embdding.txt\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader_inf):\n\u001b[1;32m---> 26\u001b[0m         embedding_feats \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         embedding_feats \u001b[38;5;241m=\u001b[39m embedding_feats\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m feat, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embedding_feat, labels):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mT:\\Git_repo\\Face_Recognition\\face_embedding\\model\\iresnet.py:133\u001b[0m, in \u001b[0;36mIResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 133\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "if not args.cpu_mode:\n",
    "    model = model.cuda()\n",
    "    \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((112, 112)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "\n",
    "data_inf = dataset.FaceImageFolderDataset(root= args.folderdataset_dir, transform = transform)\n",
    "dataloader_inf=DataLoader(data_inf, \n",
    "                          batch_size= args.batch_size, \n",
    "                          num_workers= args.workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False,\n",
    ")\n",
    "cprint('=> starting face embdding...', 'green')\n",
    "cprint('=> embdding feature will be saved into {}'.format(args.feat_list))\n",
    "model.eval()\n",
    "\n",
    "file= open(args.feat_list, 'w')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(dataloader_inf):\n",
    "        imgs=imgs.cuda()\n",
    "        embedding_feats = model(imgs)\n",
    "        embedding_feats = embedding_feats.data.cpu().numpy()\n",
    "        \n",
    "        for feat, label in zip(embedding_feat, labels):\n",
    "            file.write('{} '.format(label))\n",
    "            for r in feat:\n",
    "                file.write('{} '.format(r))\n",
    "            file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbce62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb361bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "  (prelu): PReLU(num_parameters=64)\n",
       "  (layer1): Sequential(\n",
       "    (0): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=64)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=64)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=128)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=128)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=256)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=256)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=512)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): IBasicBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=512)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout2d(p=0.4, inplace=True)\n",
       "  (fc): Linear(in_features=25088, out_features=512, bias=True)\n",
       "  (features): BatchNorm1d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e0701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
